{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://127.0.0.1:8123\" \n",
    "os.environ['https_proxy'] = \"http://df-127.0.0.1:8123\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(True)\n",
    "\n",
    "### LLM\n",
    "\n",
    "local_llm = 'llama3'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 0}, page_content='An Image is Worth 32 Tokens\\nfor Reconstruction and Generation\\nQihang Yu1*, Mark Weber1,2*, Xueqing Deng1, Xiaohui Shen1, Daniel Cremers2, Liang-Chieh Chen1\\n1ByteDance2Technical University Munich * equal contribution\\nhttps://yucornetto.github.io/projects/titok.html\\n32 tokensTiTok(ours)256 tokensVQGAN65536 pixelsrealimagelatent size and costsImage Reconstruction32 tokens can work well for‚Ä¶\\nImage Generation(TiTok32tokens)'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 0}, page_content='Image Generation(TiTok32tokens)\\nFigure 1: We propose TiTok , a compact 1Dtokenizer leveraging region redundancy to represent an\\nimage with only 32tokens for image reconstruction and generation.\\nAbstract\\nRecent advancements in generative models have highlighted the crucial role of\\nimage tokenization in the efficient synthesis of high-resolution images. Tokeniza-\\ntion, which transforms images into latent representations, reduces computational'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 0}, page_content='demands compared to directly processing pixels and enhances the effectiveness\\nand efficiency of the generation process. Prior methods, such as VQGAN, typically\\nutilize 2D latent grids with fixed downsampling factors. However, these 2D tok-\\nenizations face challenges in managing the inherent redundancies present in images,\\nwhere adjacent regions frequently display similarities. To overcome this issue,\\nwe introduce Transformer-based 1-D imensional Tokenizer (TiTok), an innovative'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 0}, page_content='approach that tokenizes images into 1D latent sequences. TiTok provides a more\\ncompact latent representation, yielding substantially more efficient and effective\\nrepresentations than conventional techniques. For example, a 256√ó256√ó3image\\ncan be reduced to just 32discrete tokens, a significant reduction from the 256 or\\n1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves\\ncompetitive performance to state-of-the-art approaches. Specifically, using the'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 0}, page_content='same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT\\nbaseline significantly by 4.21 at ImageNet 256√ó256benchmark. The advantages\\nof TiTok become even more significant when it comes to higher resolution. At\\nImageNet 512√ó512benchmark, TiTok not only outperforms state-of-the-art dif-\\nfusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens\\nby64√ó, leading to 410√ófaster generation process. Our best-performing variant'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 0}, page_content='can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating\\nhigh-quality samples 74√ófaster .\\nPreprint. Under review.arXiv:2406.07550v1  [cs.CV]  11 Jun 2024'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='1 Introduction\\nIn recent years, image generation has experienced remarkable progress, driven by the significant\\nadvancements in both transformers [ 19,62,66,10,67,68] and diffusion models [ 16,55,29,49,21].\\nMirroring the trends in generative language models [ 48,59], the architecture of many contemporary\\nimage generation models incorporate a standard image tokenizer and de-tokenizer. This array of\\nmodels utilizes tokenized image representations‚Äîranging from continuous [ 34] to discrete vec-'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='tors [ 54,61,19]‚Äîto perform a critical function: translating raw pixels into a latent space. The latent\\nspace ( e.g.,32√ó32) is significantly more compact than the original image space ( 256√ó256√ó3). It\\noffers a compressed yet expressive representation, and thus not only facilitates efficient training and\\ninference of generative models but also paves the way to scale up the model size.\\nAlthough image tokenizers achieve great success in image generation workflows, they encounter a'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='fundamental limitation tied to their intrinsic design. These tokenizers are based on an assumption\\nthat the latent space should retain a 2D structure, to maintain a direct mapping for locations between\\nthe latent tokens and image patches. For example, the top-left latent token directly corresponds to\\nthe top-left image patch. This restricts the tokenizer‚Äôs ability to effectively leverage the redundancy\\ninherent in images to cultivate a more compressed latent space.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='Taking one step back, we raise the question ‚Äúis 2D structure necessary for image tokenization?‚Äù\\nTo answer the question, we draw inspiration from several image understanding tasks where model\\npredictions are based solely on high-level information extracted from input images ‚Äîsuch as in image\\nclassification [ 17], object detection [ 8,77], segmentation [ 64,71], and multi-modal large language\\nmodels [ 1,40]. These tasks do not need de-tokenizers, since the outputs typically manifest in specific'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='structures other than images. In other words, they often format a higher-level 1D sequence as output\\nthat can still capture all task-relevant information. Prior arts, such as object queries [ 8,64] or the\\nperceiver resampler [ 1], encode images into a 1D sequence of a predetermined number of tokens ( e.g.,\\n64). These tokens facilitate the generation of outputs like bounding boxes or captions. The success of'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='these methods motivates us to investigate a more compact 1D sequence as image latent representation\\nin the context of image reconstruction and generation. It is noteworthy that the synthesis of both\\nhigh-level and low-level information is crucial for the generation of high-quality images, providing a\\nchallenge for extremely compact latent representations.\\nIn this work, we introduce a transformer-based framework [ 62,17] designed to tokenize an image'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='to a 1D discrete sequence, which can later be decoded back to the image space via a de-tokenizer.\\nSpecifically, we present Transformer-based 1-D imensional Tokenizer (TiTok), consisting of a Vision\\nTransformer (ViT) encoder, a ViT decoder, and a vector quantizer following the typical Vector-\\nQuantized (VQ) model designs [ 19]. In the tokenization phase, the image is split and flattened into a\\nseries of patches, followed by concatenation with a 1D sequence of latent tokens. After the feature'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='encoding process of ViT encoder, these latent tokens build the latent representation of the image.\\nSubsequent to the vector quantization step [ 61,19], the ViT decoder is utilized to reconstruct the\\ninput images from the masked token sequence [15, 24].\\nBuilding upon TiTok, we conduct extensive experiments to probe the dynamics of 1D image tokeniza-\\ntion. Our investigation studies the interplay between latent space size, model size, reconstruction'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='fidelity, and generative quality. From this exploration, several compelling insights emerge:\\n1.Increasing the number of latent tokens representing an image consistently improves the\\nreconstruction performance, yet the benefit becomes marginal after 128 tokens. Intriguingly,\\n32 tokens are sufficient for a reasonable image reconstruction.\\n2.Scaling up the tokenizer model size significantly improves performance of both reconstruc-'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='tion and generation, especially when number of tokens is limited ( e.g., 32 or 64), showcasing\\na promising pathway towards a compact image representation at latent space.\\n3.1D tokenization breaks the grid constraints in prior 2D image tokenizers, which not only\\nenables each latent token to reconstruct regions beyond a fixed image grid and leads to a\\nmore flexible tokenizer design, but also learns more high-level and semantic-rich image\\ninformation, especially at a compact latent space.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 1}, page_content='4.1D tokenization exhibits superior performance in generative training, with not only a signifi-\\ncant speed-up for both training and inference but also a competitive FID score compared to\\na typical 2D tokenizer, while using much fewer tokens.\\n2'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='MaskGITU-ViT-H/4DiT-XL/2U-ViT-L/4fastersamplespeed\\nbettergenerationquality(a)Modelcomparisononresolutionof256(b)Modelcomparisononresolutionof512\\nADMViT-VQGANLDM-4U-ViT-H/2DiT-XL/2TiTok-S-128TiTok-B-64150xfaster13xfasterU-ViT-L/2fastersamplespeed\\nbettergenerationqualityFIDlowerbound(valset)\\n1.78333xfaster410xfasterTiTok-L-64TiTok-B-128TiTok-L-32169xfaster\\n1.69FIDlowerbound(valset)Figure 2: A speed and quality comparison of TiTok and prior arts on ImageNet 256√ó256and'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='512√ó512generation benchmarks. Speed-up is compared against DiT-XL/2 [ 49]. The sampling\\nspeed (de-tokenization included) is measured with an A100 GPU.\\nIn light of these findings, we introduce the TiTok family, encompassing models of varying model\\nsizes and latent sizes, capable of achieving highly compact tokenization with as few as 32 tokens . We\\nfurther confirm the model‚Äôs efficacy in image generation through the MaskGIT [ 9] framework. TiTok'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='is demonstrated to facilitate state-of-the-art performance in image generation, while requiring latent\\nspaces that are 8√óto64√ósmaller, resulting in significant accelerations during both the training and\\ninference phases. It also generates images with similar or higher quality but up to 410√ófaster than\\nstate-of-the-art diffusion models such as DiT [18] (Fig. 2).\\n2 Related Work\\nImage Tokenization. Images have been compressed since the early days of deep learning with'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='autoencoders [ 27,63]. The general design of using an encoder that compresses high-dimensional\\nimages into a low-dimensional latent representation and then using a decoder to reverse the process,\\nhas proven to be successful over the years. Variational Autoencoders (V AEs) [ 34] extend the paradigm\\nby learning to map the input to a distribution. Instead of modeling a continuous distribution, VQ-'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='V AEs [ 47,53] learn a discrete representation forming a categorical distribution. VQGAN [ 19] further\\nimproves the training process by using adversarial training [ 23]. The transformer design of the\\nautoencoder is further explored in ViT-VQGAN [ 65] and Efficient-VQGAN [ 7]. Orthogonal to this,\\nRQ-V AE [ 36] and MoVQ [ 76] study the effect of using multiple vector quantization steps per latent\\nembedding, while MAGVIT-v2 [ 68] and FSQ [ 45] propose a lookup-free quantization. However,'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='allaforementioned works share the same workflow of an image always being patchwise encoded\\ninto a 2Dgrid latent representation. In this work, we explore an innovative 1Dsequence latent\\nrepresentation for image reconstruction and generation.\\nTokenization for Image Understanding. For image understanding tasks ( e.g., image classifica-\\ntion [ 17], object detection [ 8,77,74], segmentation [ 64,70,72], and Multi-modal Large Language'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='Models (MLLMs) [ 1,40,73]), it is common to use a general feature encoder instead of an autoen-\\ncoder to tokenize the image. Specifically, many MLLMs [ 40,42,58,32,22,11] uses a CLIP [ 51]\\nencoder to tokenize the image into highly semantic tokens, which proves effective for image cap-\\ntioning [ 13] and VQA [ 2]. Some MLLMs also explore discrete tokens [ 32,22] or ‚Äúde-tokenize‚Äù the\\nCLIP embeddings back to images through diffusion models [ 58,32,22]. However, due to the nature'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='of CLIP models that focus on high-level information, these methods can only reconstruct an image\\nwith high-level semantic similarities ( i.e., the layouts and details are not well-reconstructed due to\\nCLIP features). Therefore, our method is significantly different from theirs, since the proposed TiTok\\naims to reconstruct both the high-level and low-level details of an image, same as typical VQ-V AE\\ntokenizers [34, 54, 19].'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='tokenizers [34, 54, 19].\\nImage Generation. Image generation methods range from sampling the V AE [ 34], using GANs [ 23]\\nto Diffusion Models [ 16,55,29,49,21] and autoregressive models [ 60,12,47]. Prior studies that\\nare most related to this work build on top of a learned VQ-V AE codebook to generate images.\\nAutoregressive transformer [ 19,65,7,36], similar to decoder-only language models, model each'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 2}, page_content='patch in a step-by-step fashion, thus requiring as many steps as token number, e.g., 256 or 1024.\\n3'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='. . .mask tokens\\nVisionTransformer\\n. . .image patches. . .latent tokens. .codebook01N\\n. . .. . .input\\nreconstructionùë∏ùíñùíÇùíèùíï. . .1190ùë´ùíÜùíÑVisionTransformerùë¨ùíèùíÑ\\n(a)ImageReconstructionK(e.g.,32)latenttokensinputTiTokencoderTiTokdecoderVQreconstruction\\n(b)ImageGenerationlatenttokensmaskedtokensBidirectionalTransformerpredictedtokens(c)TiTok Tokenization1. . .1901. . .1901. . .01. . .190Figure 3: Illustration of image reconstruction (a) and generation (b) with the TiTok framework'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='(c).TiTok contains an encoder Enc, a quantizer Quant , and a decoder Dec. Image patches, along\\nwith a few ( e.g., 32) latent tokens, are passed through the Vision Transformer (ViT) encoder. The\\nlatent tokens are then vector-quantized. The quantized tokens, along with the mask tokens [ 15,24],\\nare fed to the ViT decoder to reconstruct the image.\\nNon-autoregressive (or bidirectional) transformers [ 76,68], such as MaskGIT [ 9], generally predict'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='more than a single token per step and thus require significantly fewer steps to predict a complete\\nimage. Apart from that, further studies looked into improved sampling strategies [ 38,39,37]. As we\\nfocus on the tokenization stage, we apply the commonly used non-autoregressive sampling scheme\\nof MaskGIT to generate a sequence of tokens that is later decoded into an image.\\n3 Method\\n3.1 Preliminary Background on VQ-V AE'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='3 Method\\n3.1 Preliminary Background on VQ-V AE\\nThe image tokenizer plays a pivotal role in facilitating the generative models by providing a compact\\nimage representation at latent space. For the scope of our discussion, we primarily focus on the Vector-\\nQuantized (VQ) tokenizer [ 61,19], given its broad applicability across various domains, including\\nbut not limited to image and video generation [ 19,9,55,67], large-scale pretraining [ 12,5,46,3,18]\\nand multi-modal models [20, 69].'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='and multi-modal models [20, 69].\\nA typical VQ model contains three key components: an encoder Enc, a vector quantizer Quant , and\\na decoder Dec. Given an input image I‚ààRH√óW√ó3, where HandWdenote the image‚Äôs height\\nand width, the image is initially processed by the encoder Enc and converted to latent embeddings\\nZ2D=Enc(I), where Z2D‚ààRH\\nf√óW\\nf√óD, which downsamples the spatial dimensions by a factor of\\nf. Subsequently, each embedding z‚ààRDis mapped (via the vector quantizer Quant ) to the nearest'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='codeci‚ààRDin a learnable codebook C‚ààRN√óD, comprising Ncodes. Formally, we have:\\nQuant (z) =ci,where i= argmin\\nj‚àà{1,2,...,N}‚à•z‚àícj‚à•2. (1)\\nDuring de-tokenization, the reconstructed image ÀÜIis obtained via the decoder Dec as follows:\\nÀÜI=Dec(Quant (Z2D)). (2)\\nDespite the numerous improvements over VQ-V AE [ 61] (e.g., loss function [ 19], model architec-\\nture [ 65], and quantization/codebook strategies [ 76,36,68]), the fundamental workflow ( e.g., the 2D'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='grid-based latent representations) has largely remained unchanged.\\n3.2 TiTok: From 2D to 1D Tokenization\\nWhile existing VQ models have demonstrated significant achievements, a notable limitation within the\\nstandard workflow exists: the latent representation Z2Dis often envisioned as a static 2D grid. Such a\\nconfiguration inherently assumes a strict one-to-one mapping between the latent grids and the original'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 3}, page_content='image patches. This assumption limits the VQ model‚Äôs ability to fully exploit the redundancies\\npresent in images, such as similarities among adjacent patches. Additionally, this approach constrains\\n4'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='the flexibility in selecting the latent size, with the most prevalent configurations being f= 4,f= 8,\\norf= 16 [55], resulting in 4096 ,1024 , or256tokens for an image of dimensions 256√ó256√ó3.\\nInspired by the success of 1D sequence representations in addressing a broad spectrum of computer\\nvision problems [ 8,1,40], we propose to use a 1D sequence, without the fixed correspondence\\nbetween latent representation and image patches in 2D tokenization, as an efficient and effective'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='latent representation for image reconstruction and generation.\\nImage Reconstruction with TiTok. To initiate our exploration, we establish a novel frame-\\nwork named Transformer-based 1-D imensional Tokenizer (TiTok), leveraging Vision Transformer\\n(ViT) [ 17]1to tokenize images into 1D latent tokens and subsequently reconstruct the original images\\nfrom these 1D latents. As depicted in Fig. 3, TiTok employs a standard ViT for both the tokenization'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='and de-tokenization processes ( i.e., both the encoder Enc and decoder Dec are ViTs). During\\ntokenization, we patchify the image into patches (with a patch embedding layer) P‚ààRH\\nf√óW\\nf√óD\\n(with patch size equal to the downsampling factor fand embedding dimension D) and concatenate\\nthem with Klatent tokens L‚ààRK√óD. They are then fed into the ViT encoder Enc. In the encoder\\noutput, we only retain the latent tokens as the image‚Äôs latent representation, thereby enabling a more'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='compact latent representation of 1D sequence Z1D(with length K). This adjustment decouples the\\nlatent size from image‚Äôs resolution and allows more flexibility in design choices. That is, we have:\\nZ1D=Enc(P‚äïL), (3)\\nwhere ‚äïdenotes concatenation, and we only retain the latent tokens from the encoder output.\\nIn the de-tokenization phase, drawing inspiration from [ 15,5,24], we incorporate a sequence of\\nmask tokens M‚ààRH\\nf√óW\\nf√óD‚Äîobtained by replicating a single mask tokenH\\nf√óW\\nftimes‚Äîto the'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='f√óW\\nftimes‚Äîto the\\nquantized latent tokens Z1D. The image is then reconstructed via the ViT decoder Dec as follows:\\nÀÜI=Dec(Quant (Z1D)‚äïM), (4)\\nwhere the latent tokens Z1Dis first vector-quantized by Quant and then concatenated with the mask\\ntokens Mbefore feeding to the decoder Dec.\\nDespite its simplicity, we emphasize that the concept of compact 1D image tokenization remains\\nunderexplored in existing literature. The proposed TiTok thus serves as a foundational platform for'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='exploring the potentials of 1D tokenization and de-tokenization for natural images. It is worth noting\\nthat although one may flatten 2D grid latents into a 1D sequence, it significantly differs from the\\nproposed 1D tokenizer, due to the fact that the implicit 2D grid mapping constraints still persist.\\nImage Generation with TiTok. Besides the image reconstruction task which the tokenizer is\\ntrained for, we also evaluate its effectiveness for image generation, following the typical pipeline [ 19,'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='9]. Specifically, we adopt MaskGIT [ 9] as our generation framework due to its simplicity and\\neffectiveness, allowing us to train a MaskGIT model by simply replacing its VQGAN tokenizer with\\nour TiTok. We do not make any other specific modifications to MaskGIT, but for completeness, we\\nbriefly describe its whole generation process with TiTok.\\nThe image is pre-tokenized into 1D discrete tokens. At each training step, a random ratio of the latent'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='tokens are replaced with mask tokens. Then, a bidirectional transformer takes the masked token\\nsequence as input, and predicts the corresponding discrete token ID of those masked tokens. The\\ninference process consists of multiple sampling steps, where at each step the transformer‚Äôs prediction\\nfor masked tokens will be sampled based on the prediction confidence, which are then used to update\\nthe masked images. In this way, the image is ‚Äúprogressively generated‚Äù from a sequence full of'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='mask tokens to an image with generated tokens, which can later be de-tokenized back into pixel\\nspaces. The MaskGIT framework shows a significant speed-up in the generation process compared to\\nauto-regressive models. We refer readers to [9] for more details.\\n3.3 Two-Stage Training of TiTok with Proxy Codes\\nExisting Training Strategies for VQ Models. Although most VQ models adhere to a straightforward\\nformulation, their training process is notably sensitive, and the model‚Äôs performance is heavily'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 4}, page_content='influenced by the adoption of more effective training paradigms. For instance, VQGAN [ 19] achieves\\n1Although other Transformer-based architectures ( e.g., Swin [ 43]) can also be used to instantiate TiTok, we\\nadopt ViT for its simplicity and effectiveness.\\n5'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='a significant improvement in reconstruction FID (rFID) on the ImageNet [ 14] validation set, when\\ncompared to dV AE from DALL-E [ 52]. This enhancement is attributed to advancements in perceptual\\nloss [ 33,75] and adversarial loss [ 23]. Moreover, MaskGIT‚Äôs modern implementation of VQGAN [ 9]\\nutilizes refined training techniques without architectural improvements to boost the performance\\nfurther. Notably, most of these improvements are exclusively applied during the training phase ( i.e.,'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='through auxiliary losses) and significantly affect the models‚Äô efficacy. Given the complexity of the\\nloss functions, extensive tuning of hyper-parameters involved, and, most critically, the missing of\\na publicly available code-base for reference or reproduction [ 9,65,68], establishing an optimal\\nexperimental setup for the proposed TiTok presents a substantial challenge, especially when the target\\nis a compact 1D tokenization which was rarely studied in literature.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='Two-Stage Training Comes to the Rescue. Although training TiTok with the typical Taming-\\nVQGAN [ 19] setting is feasible, we introduce a two-stage training paradigm for an improved\\nperformance. The two-stage training strategy contains ‚Äúwarm-up‚Äù and ‚Äúdecoder fine-tuning‚Äù stages.\\nSpecifically, in the first ‚Äúwarm-up‚Äù stage, instead of directly regressing the RGB values and employing\\na variety of loss functions (as in existing methods), we propose to train 1D VQ models with the discrete'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='codes generated by an off-the-shelf MaskGIT-VQGAN model, which we refer to as proxy codes . This\\napproach allows us to bypass the intricate loss functions and GAN architectures, thereby concentrating\\nour efforts on optimizing the 1D tokenization settings. Importantly, this modification does not harm\\nthe functionality of the tokenizer and quantizer within TiTok, which can still fully function for image\\ntokeniztion and de-tokenization; the main adaptation simply involves the processing of TiTok‚Äôs'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='de-tokenizer output. Specifically, this output, comprising a set of proxy codes, is subsequently fed\\ninto the same off-the-shelf VQGAN decoder to generate the final RGB outputs. It is noteworthy that\\nthe introduction of proxy codes differs from a simple distillation [ 26]. As verified in our experiments,\\nTiTok yields significantly better generation performance than MaskGIT-VQGAN.\\nAfter the first training stage with proxy codes, we optionally have the second ‚Äúdecoder fine-tuning‚Äù'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='stage, inspired by [ 10,50], to improve the reconstruction quality. Specifically, we keep the encoder\\nand quantizer frozen, and only train the decoder towards pixel space with the typical VQGAN training\\nrecipe [ 19]. We observe that such a two-stage training strategy significantly improves the training\\nstability and reconstructed image quality, as shown in the experiments.\\n4 Experimental Results\\n4.1 Preliminary Experiments of 1D Tokenization'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='4.1 Preliminary Experiments of 1D Tokenization\\nBuilding upon TiTok, we explore a range of configurations, including the model size and the number\\nof tokens, to identify the most efficient and effective setup for a 1D image tokenizer. These preliminary\\nexperiments serve to provide a thorough evaluation, seeking a practical configuration of TiTok.\\nPreliminary Experimental Setup. Unless specified otherwise, we train all models with images'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='of resolution H= 256 andW= 256 , using the open-source MaskGIT-VQGAN [ 9] to supply\\nproxy codes for training. The patch size for both tokenizer and de-tokenizer is established with\\nf= 16 , and the codebook Cis configured to have N= 1024 entries with each entry a vector with\\n16channels. For TiTok variants, we primarily investigate three model sizes‚Äîsmall, base, and large\\n(i.e., TiTok-S, TiTok-B, TiTok-L)‚Äîcomprising 22M,86M, and 307Mparameters for encoder and'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='decoder, respectively. We also assess the impact of varying the number of latent tokens Kfrom 16to\\n256. We perform ablation experiments with an efficient setting ( e.g., shorter training).\\nEvaluation Protocol. Evaluation is conducted across multiple metrics to thoroughly assess the\\nmodels, including both reconstruction and generation FID metrics ( i.e., rFID and gFID) [ 25] on\\nthe ImageNet dataset. We examine training/inference throughput to offer a direct comparison of'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='generative model‚Äôs efficiency relative to different latent sizes. Furthermore, given that the 1D VQ\\nmodel inherently serves as a form of compact image compression, we further investigate the semantic\\ninformation retained by the model through linear probing following MAE setting [ 24]. For the\\ncomplete details of the training and testing protocols ( e.g., hyper-parameters, training costs), we refer\\nthe reader to the supplementary material Sec. A.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 5}, page_content='the reader to the supplementary material Sec. A.\\nAfter the setup, we now summarize the preliminary experimental findings below.\\nAn Image Can be Represented by 32 Tokens. The redundancy inherent in image representation\\nis well-acknowledged, as evidenced by the practice of masking significant portions of images ( e.g.,\\n6'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='1632 64 96 128 192 256\\nnumber of latent tokens510152025reconstruction FID (rFID)\\n(a) IN-1K Reconstruction1632 64 96 128 192 256\\nnumber of latent tokens45.047.550.052.555.057.560.062.5linear probe accuracy (%)\\n(b) IN-1K Linear Probe1632 64 96 128 192 256\\nnumber of latent tokens46810121416generation FID (gFID)\\n(c) IN-1K Generation1632 64 96 128 192 256\\nnumber of latent tokens100020003000400050006000training throughput (samples/s/gpu)\\n5743.3\\n2815.2\\n1268.3\\n824.8\\n555.1\\n327.0\\n219.7'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='5743.3\\n2815.2\\n1268.3\\n824.8\\n555.1\\n327.0\\n219.7\\n(d) Sampling Speed20406080100120140160\\ninference throughput (samples/s/gpu)\\n160.0\\n123.1\\n89.8\\n70.9\\n54.1\\n39.0\\n27.5TiT ok-S TiT ok-B TiT ok-L T aming-VQGAN MaskGIT-VQGANFigure 4: Preliminary experimental results with different TiTok variants. We provide a com-\\nprehensive exploration in (a) ImageNet-1K reconstruction. (b) ImageNet-1K linear probing. (c)\\nImageNet-1K generation. (d) Training and inference throughput of MaskGIT-ViT as generator and'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='TiTok as tokenizer (evaluated on A100 GPUs, inference includes de-tokenization step with TiTok-B).\\nDetailed numbers can be found in supplementary material Sec. B.\\n75% in MAE [ 24]) to expedite the training process without negatively affecting performance. This\\nstrategy has been validated across a variety of computer vision tasks that rely on high-level image\\nfeatures [ 30,41]. However, the efficacy of such approaches in the context of image reconstruction'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='and generation‚Äîwhere both low-level and high-level details are crucial for creating realistic recon-\\nstructed and generated outputs‚Äîremains underexplored. Consequently, in this experiment, we aim to\\ndetermine the minimum number of tokens required to reconstruct and generate high-quality images.\\nAs depicted in Fig. 4a, although model performance progressively improves with an increase in the\\nnumber of latent tokens, significant enhancements are predominantly observed when Kranges from'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='16to128. Beyond this point, increasing the latent space size yields only marginal gains. Intriguingly,\\nwe find that with merely 32latent tokens, TiTok-L achieves performance better than a 2D VQGAN\\nmodel [ 19] using 256 tokens . This observation suggests that as few as 32tokens may suffice as an\\neffective image latent representation, optimizing the utilization of image redundancy.\\nScaling Up Tokenizer Enables More Compact Latent Size. Another intriguing observation from'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='Fig. 4a is that larger tokenizers facilitate more compact representations. Specifically, TiTok-B with 64\\nlatent tokens achieves performance comparable to TiTok-S with 128latent tokens, while TiTok-L with\\n32latent tokens matches the performance of TiTok-B with 64latent tokens. This pattern indicates\\nthat with each incremental increase in TiTok size ( e.g., from S to B, or from B to L), it is possible to'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='reduce the size of the latent image representation without compromising performance. This trend\\nunderscores the potential benefits of scaling up the tokenizer to achieve even more compact image\\nrepresentations.\\nSemantics Emerges with Compact Latent Space. To evaluate the learned image representation, we\\nperform linear probing experiments on the image tokenizer, as shown in Fig. 4b. Specifically, we'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='add a batch normalization layer [ 31] followed by a linear layer on top of the frozen features from\\nTiTok encoder, with all hyper-parameters strictly following the MAE protocol [ 24].We find that as\\nthe size of the latent representation decreases, the tokenizer increasingly learns semantically rich\\nrepresentations , as indicated by the improved linear probing accuracy. This suggests that the model\\nlearns high-level information in scenarios of constrained representation space.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='Compact Latent Representation Improves Generative Training. In addition to reconstruction\\ncapabilities, we assess TiTok‚Äôs effectiveness and efficiency in generative downstream tasks, as\\nillustrated in Fig. 4c and Fig. 4d. We note that variants of different tokenizer sizes yield comparable\\noutcomes when the number of latent tokens is sufficiently large ( i.e.,K‚â•128). However, within\\nthe domain of compact latent sizes ( i.e.,K‚â§64), larger tokenizers notably enhance performance.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='Furthermore, the adaptability of 1D tokenization in TiTok facilitates more efficient and effective\\ngenerative model training. For instance, model variants with K= 32 , despite inferior reconstruction\\nquality, demonstrate significantly better generative performance, underscoring the advantages of\\nemploying a more condensed and semantically rich latent space for generative model training .\\nAdditionally, the reduction in latent tokens markedly accelerates training and inference, with a 12.8√ó'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 6}, page_content='increase in training speed (2815.2 vs. 219.7 samples/s/gpu) and a 4.5√óspeed up sampling speed\\n(123.1 vs. 27.5 samples/s/gpu), when utilizing K= 32 as opposed to K= 256 .\\n7'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='Table 1: ImageNet-1K 256√ó256generation results evaluated with ADM [ 16].‚Ä†: Trained on\\nOpenImages [ 35]‚Ä°: Trained on OpenImages, LAION-Aesthetics/-Humans [ 56]. P: generator‚Äôs\\nparameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.\\ntokenizer #tokens codebook size rFID ‚Üì generator gFID ‚ÜìP‚Üì S‚ÜìT‚Üë\\ndiffusion-based generative models\\nTaming-VQGAN‚Ä† [55] 1024 16384 1.14 LDM-8 [55] 7.76 258M 200 -\\nV AE‚Ä† [55] 4096 √ó3 - 0.27 LDM-4 [55] 3.60 400M 250 0.4'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='V AE [57]‚Ä° 1024 √ó4 - 0.62UViT-L/2 [4] 3.40 287M 50 1.1\\nUViT-H/2 [4] 2.29 501M 50 0.6\\nDiT-XL/2 [49] 2.27 675M 250 0.6\\ntransformer-based generative models\\nTaming-VQGAN [19] 256 1024 7.94 Taming-Transformer [19] 15.78 1.4B 256 7.5\\nRQ-V AE [36] 256 16384 3.20 RQ-Transformer [36]8.71 1.4B6416.1\\n7.55 3.8B 9.7\\nMaskGIT-VQGAN [9] 256 1024 2.28 MaskGIT-ViT [9] 6.18 177M 8 50.5\\nViT-VQGAN [65] 1024 8192 1.28 VIM-Large [65] 4.17 1.7B 1024 0.3\\nTiTok-L-32 32 4096 2.21 MaskGIT-ViT [9] 2.77 177M 8 101.6'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='TiTok-B-64 64 4096 1.70 MaskGIT-ViT [9] 2.48 177M 8 89.8\\nTiTok-S-128 128 4096 1.71 MaskGIT-UViT-L [9, 4]2.50287M853.3\\n1.97 64 7.8\\n4.2 Main Experiments\\nBased on the observations above, the proposed TiTok family effectively trades off a larger model size\\nto a more compact latent size. In this section, we majorly focus on ImageNet generation benchmarks\\nagainst prior arts, and evaluate TiTok as a tokenizer in the generative MaskGIT framework [9].'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='Implementation Details. We primarily investigate the following TiTok variants: TiTok-S-128 ( i.e.,\\nsmall model with 128 tokens), TiTok-B-64 ( i.e., base model with 64 tokens), and TiTok-L-32 ( i.e.,\\nlarge model with 32 tokens), where each variant designed to halve the latent space size while scaling\\nup the model size. For resolution 512, we double the latent size to ensure more details are kept at\\nhigher resolution, leading to TiTok-L-64 and TiTok-B-128. In the final setting for TiTok training,'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='the codebook is configured to N= 4096 , and the training duration is extended to 1Miterations\\n(200 epochs). We also adopt the ‚Äúdecoder fine-tuning‚Äù stage to further enhance model performance,\\nwhere the encoder and quantizer are kept frozen and the decoder is fine-tuned for 500kiterations.\\nFor the training of generative models, we utilize the MaskGIT [ 9] framework without any specific\\nmodifications, except for the adoption of an arccos masking schedule [ 6]. All other parameters are'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='the same as previous setups, and all design improvements will be verified in the ablation studies.\\nMain Results. We summarize the results on ImageNet-1K generation benchmark of resolution\\n256√ó256and512√ó512in Tab. 1 and Tab. 2, respectively.2\\nFor ImageNet 256√ó256results in Tab. 1, TiTok can achieve a similar level of reconstruction FID\\n(rFID) with a much smaller number of latent tokens than other VQ models. Specifically, using'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='merely 32tokens, TiTok-L-32 achieves a rFID of 2.21, comparable to the well trained VQGAN\\nfrom MaskGIT [ 9] (rFID 2.28), while using 8√ósmaller latent representation size. Furthermore,\\nwhen using the same generator framework and same sampling steps, TiTok-L-32 improves over\\nMaskGIT by a large margin (from 6.18to2.77gFID), showcasing the benefits of a more effective\\ngenerator training with compact 1D tokens. When compared to other diffusion-based generative'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='models, TiTok can also achieve a competitive performance while enjoying an over 100√óspeed-up\\nduring the sampling process. Specifically, TiTok-L-32 achieves a better gFID than LDM-4 [ 55]\\n(2.77 vs. 3.60), while generating images dramatically faster by 254times (101.6 samples/s vs. 0.4\\nsamples/s). Our best-performing variant TiTok-S-128 outperforms state-of-the-art diffusion method\\nDiT-XL/2 [49] (gFID 1.97vs.2.27), with a 13√óspeed-up.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='For ImageNet 512√ó512results in Tab. 2, the significantly better accuracy-cost trade-off of TiTok\\npersists. TiTok maintains a reasonably good rFID compared to other methods, especially considering\\nthat TiTok uses much fewer tokens ( i.e., higher compression ratio). For generation, all TiTok variants\\n2For fairness, we mainly consider tokenizers with vanilla VQ modules. More advanced quantization\\nmethods [ 68,45] may further benefit TiTok but beyond this paper‚Äôs focus on 1D image tokenization. See'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 7}, page_content='supplementary material Sec. C for the complete table.\\n8'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='Table 2: ImageNet-1K 512√ó512generation results evaluated with ADM [ 16].‚Ä°: Trained on\\nOpenImages, LAION-Aesthetics and LAION-Humans [ 56]. P: generator‚Äôs parameters. S: sampling\\nsteps. T: throughput as samples per seconds on A100 with float32 precision.\\ntokenizer #tokens codebook size rFID ‚Üì generator gFID ‚ÜìP‚ÜìS‚ÜìT‚Üë\\ndiffusion-based generative models\\nV AE [57]‚Ä° 4096 √ó3 - 0.19UViT-L/4 [4] 4.67 287M 50 1.0\\nUViT-H/4 [4] 4.05 501M 50 0.6\\nDiT-XL/2 [49] 3.04 675M 250 0.1'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='DiT-XL/2 [49] 3.04 675M 250 0.1\\ntransformer-based generative models\\nMaskGIT-VQGAN [9] 1024 1024 1.97 MaskGIT-ViT [9] 7.32 177M 12 3.9\\nTiTok-L-64 64 4096 1.77 MaskGIT-ViT [9] 2.74 177M 8 41.0\\nTiTok-B-128 128 4096 1.52MaskGIT-ViT [9] 2.49 177M 8 33.3\\nMaskGIT-ViT [9] 2.13 177M 64 7.4\\nTable 3: Ablation study improved final models for main experiments. We ablate the tokenizer\\ndesigns, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='Generation results are based on tokenizers without decoder fine-tuning\\n(a) TiTok configuration. Results\\nreported in accumulation manner\\nTiTok-L-32 rFID ‚Üì IS‚Üë\\nbaseline 6.59 110.3\\n+ larger codebook 5.85 116.6\\n+ 200 epochs 5.48 117.3\\n+ decoder finetuning 2.21 195.5(b) Masking schedules for genera-\\ntor with TiTok-L-32\\nschedule gFID ‚Üì IS‚Üë\\ncosine 5.17 191.8\\narccos 4.94 194.0\\nlinear 4.95 193.7\\nsquare root 5.63 170.9(c) Effects of proxy codes\\nrFID‚Üì IS‚Üë\\nTaming-VQGAN training setting'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='rFID‚Üì IS‚Üë\\nTaming-VQGAN training setting\\nTaming-VQGAN (2D) 7.94 -\\nTiTok-B-64 (2D) 15.58 64.2\\nTiTok-B-64 5.15 120.5\\nTwo-stage training with proxy codes\\nTiTok-B-64 1.70 195.2\\nsignificantly outperform our baseline MaskGIT [ 9] by a large margin. When compared with diffusion-\\nbased models, TiTok-L-64 shows a superior performance to DiT-XL/2 [ 49] (2.74vs.3.04), while\\nrunning 410√ófaster . The best-performing variant TiTok-B-128 can significantly outperform DiT-'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='XL/2 by a large margin (2.13 vs. 3.04) but also generates high-quality samples 74√ófaster . We also\\nprovide visualization results and analysis in supplementary material Sec. D.\\n4.3 Ablation Studies\\nWe report the ablation studies regarding our final model designs in Tab. 3. Specifically, in Tab. 3a,\\nwe ablate the tokenizer designs on image reconstruction. We begin with our baseline TiTok-L-32\\nwhich attains 6.59 rFID. Employing a larger codebook size improves the rFID by 0.74, while further'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='increasing the training iterations (from 100 epochs to 200 epochs) yields another 0.37 improvement\\nof rFID. On top of that, the ‚Äúdecoder fine-tuning‚Äù (our stage-2 training strategy) can substantially\\nimprove the overall reconstruction performance to 2.21 rFID.\\nIn Tab. 3b, we examine the effects of different masking schedules for MaskGIT with TiTok. Inter-\\nestingly, unlike the original MaskGIT setting [ 9] which empirically found that the cosine masking'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='schedule significantly outperforms the other schedules, we observe that MaskGIT equipped with\\nTiTok changes the preference to the arccos or linear schedules. Additionally, unlike [ 9] which\\nreported that the root schedule performs much worse than the others, we observe that TiTok is quite\\nrobust to different masking schedules. We attribute the observations to TiTok‚Äôs ability to provide a\\nmore compact and more semantic meaningful tokens compared to 2D VQGAN, as compared to the'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='cosine masking schedule, linear and arccos schedules have a lower masking ratio in the early steps.\\nThis coincides with the observation that masking ratio is usually higher for redundant signals ( e.g.,\\n75% masking ratio in images [ 24]) while relatively lower for semantic meaningful inputs ( e.g., 15%\\nmasking ratio in languages [15]).\\nWe ablate the effects of training paradigm in Tab. 3c. We begin with the training setting of'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='Taming-VQGAN [ 19], where TiTok-B-64 obtains 5.15 rFID, outperforming the original 2D Taming-\\nVQGAN‚Äôs 7.94 rFID under the same training setting. We also show the necessity of 1D tokenization\\nby building a 2D variant of TiTok-B64, where the architecture remains the same except that image\\npatches instead of latent tokens are used as image representation. As a result, we observe that the 2D\\nvariant suffers from a much worse performance (15.58 vs. 5.15 rFID), since the fixed correspondences'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 8}, page_content='in 2D tokenization limited a reasonable reconstruction under compact latent space. This result\\n9'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 9}, page_content='demonstrates the effectiveness of the proposed 1D tokenization, especially at a much more compact\\nlatent size. Although TiTok can achieve a reasonably well performance under straightforward\\nsingle-stage training, there exists a performance gap compared to the MaskGIT-VQGAN [ 9]\\ndue to the missing of a strong training recipe, of which no public reference or access exists.\\nTherefore, we adopt the two-stage training with proxy codes , which proves to be effective and can'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 9}, page_content='outperform the MaskGIT-VQGAN (1.70 vs. 2.28 rFID). It is noteworthy that the two-stage training\\nis not that crucial to obtain a reasonable 1D tokenizer, and we believe that TiTok, with the simple\\nsingle-stage Taming-VQGAN‚Äôs training setting, could also benefit from training on a lagrer-scale\\ndataset [35] as demonstrated in [55] and we leave it for future work due to the limited compute.\\n5 Conclusion'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 9}, page_content='5 Conclusion\\nIn this paper, we have explored a compact 1D tokenization TiTok for reconstructing and generating\\nnatural images. Unlike the existing 2D VQ models that consider the image latent space as a 2D\\ngrid, we provide a more compact formulation to tokenize an image into a 1D latent sequence. The\\nproposed TiTok can represent an image with 8to64times fewer tokens than the commonly used 2D\\ntokenizers. Moreover, the compact 1D tokens not only significantly improve the generation model‚Äôs'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 9}, page_content='training and inference throughput, but also achieve a competitive FID on the ImageNet benchmarks.\\nWe hope our research can shed some light in the direction towards more efficient image representation\\nand generation models with 1D image tokenization.\\n10'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='References\\n[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\\nfew-shot learning. NeurIPS , 2022.\\n[2]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. Vqa: Visual question answering. In ICCV , 2015.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='[3]Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik,\\nand Alexei A Efros. Sequential modeling enables scalable learning for large vision models. In CVPR ,\\n2024.\\n[4]Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A\\nvit backbone for diffusion models. In CVPR , 2023.\\n[5]Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In\\nICLR , 2022.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='ICLR , 2022.\\n[6]Victor Besnier and Mickael Chen. A pytorch reproduction of masked generative image transformer. arXiv\\npreprint arXiv:2310.14400 , 2023.\\n[7]Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-vqgan:\\nTowards high-resolution image generation with efficient vision transformers. In ICCV , 2023.\\n[8]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='Zagoruyko. End-to-end object detection with transformers. In ECCV , 2020.\\n[9]Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image\\ntransformer. In CVPR , 2022.\\n[10] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\\nKevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked\\ngenerative transformers. In ICML , 2023.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='generative transformers. In ICML , 2023.\\n[11] Jienneg Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable\\nvision models in the vision-language era. arXiv preprint arXiv:2404.02132 , 2024.\\n[12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\\nGenerative pretraining from pixels. In ICML , 2020.\\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\\narXiv:1504.00325 , 2015.\\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR , 2009.\\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. In NAACL , 2018.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS , 2021.\\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR ,\\n2021.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='2021.\\n[18] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal\\nShankar, Joshua M Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image\\nmodels. arXiv preprint arXiv:2401.08541 , 2024.\\n[19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.\\nInCVPR , 2021.\\n[20] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene:'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 10}, page_content='Scene-based text-to-image generation with human priors. In ECCV , 2022.\\n[21] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a\\nstrong image synthesizer. In ICCV , 2023.\\n11'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='[22] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see\\nand draw with seed tokenizer. In ICLR , 2024.\\n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\\nCourville, and Yoshua Bengio. Generative adversarial nets. NeurIPS , 2014.\\n[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders\\nare scalable vision learners. In CVPR , 2022.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='are scalable vision learners. In CVPR , 2022.\\n[25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\\ntrained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS , 2017.\\n[26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\\npreprint arXiv:1503.02531 , 2015.\\n[27] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='science , 313(5786), 2006.\\n[28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 ,\\n2022.\\n[29] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high\\nresolution images. In ICML , 2023.\\n[30] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image\\npixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849 , 2020.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\\ninternal covariate shift. In ICML , 2015.\\n[32] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song,\\nXiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. In\\nICLR , 2024.\\n[33] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='super-resolution. In ECCV , 2016.\\n[34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR , 2014.\\n[35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified\\nimage classification, object detection, and visual relationship detection at scale. IJCV , 2020.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='[36] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image\\ngeneration using residual quantization. In CVPR , 2022.\\n[37] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise: Effective\\nimage generation with contextual rq-transformer. NeurIPS , 2022.\\n[38] Jos√© Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with token-\\ncritic. In ECCV , 2022.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='critic. In ECCV , 2022.\\n[39] Jos√© Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Predictor-corrector\\nsampling for discrete diffusion models. In ICLR , 2023.\\n[40] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\\nwith frozen image encoders and large language models. In ICML , 2023.\\n[41] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 11}, page_content='pre-training via masking. In CVPR , 2023.\\n[42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS , 2023.\\n[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\\ntransformer: Hierarchical vision transformer using shifted windows. In ICCV , 2021.\\n[44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\n12'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='[45] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization:\\nVQ-V AE made simple. In ICLR , 2024.\\n[46] David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir\\nZamir. 4m: Massively multimodal masked modeling. NeurIPS , 2023.\\n[47] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.\\nNeurIPS , 2017.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='NeurIPS , 2017.\\n[48] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.\\n[49] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR , 2023.\\n[50] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna,\\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\\npreprint arXiv:2307.01952 , 2023.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='preprint arXiv:2307.01952 , 2023.\\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\\ntransferable visual models from natural language supervision. In ICML , 2021.\\n[52] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and\\nIlya Sutskever. Zero-shot text-to-image generation. In ICML , 2021.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='[53] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.\\nNeurIPS , 2019.\\n[54] Jason Tyler Rolfe. Discrete variational autoencoders. In ICLR , 2017.\\n[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution\\nimage synthesis with latent diffusion models. In CVPR , 2022.\\n[56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\\ndataset for training next generation image-text models. NeurIPS , 2022.\\n[57] stabilityai, 2023. URL https://huggingface.co/stabilityai/sd-vae-ft-ema .\\n[58] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing\\nLiu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. In ICLR , 2024.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='[59] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\\n[60] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional\\nimage generation with pixelcnn decoders. NeurIPS , 2016.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='[61] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS , 2017.\\n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. NeurIPS , 2017.\\n[63] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing\\nrobust features with denoising autoencoders. In Proceedings of the 25th international conference on'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='Machine learning , pages 1096‚Äì1103, 2008.\\n[64] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end\\npanoptic segmentation with mask transformers. In CVPR , 2021.\\n[65] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,\\nJason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR ,\\n2022.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 12}, page_content='2022.\\n[66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich\\ntext-to-image generation. TMLR , 2022.\\n13'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 13}, page_content='[67] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos√© Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann,\\nMing-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR ,\\n2023.\\n[68] Lijun Yu, Jos√© Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng,\\nAgrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion‚Äìtokenizer is key\\nto visual generation. In ICLR , 2024.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 13}, page_content='to visual generation. In ICLR , 2024.\\n[69] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu,\\nBinh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining\\nand instruction tuning. arXiv preprint arXiv:2309.02591 , 2023.\\n[70] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 13}, page_content='Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In\\nCVPR , 2022.\\n[71] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and\\nLiang-Chieh Chen. k-means Mask Transformer. In ECCV , 2022.\\n[72] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-\\nvocabulary segmentation with single frozen convolutional clip. NeurIPS , 2023.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 13}, page_content='[73] Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Towards open-ended visual recognition with large\\nlanguage model. arXiv preprint arXiv:2311.08400 , 2023.\\n[74] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung\\nShum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint\\narXiv:2203.03605 , 2022.\\n[75] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 13}, page_content='effectiveness of deep features as a perceptual metric. In CVPR , 2018.\\n[76] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors\\nfor high-fidelity image generation. NeurIPS , 2022.\\n[77] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\\ntransformers for end-to-end object detection. In ICLR , 2020.\\n14'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='Appendix\\nIn the supplementary materials, we provide the following additional details:\\n‚Ä¢The comprehensive training and testing hyper-parameters and training costs for TiTok\\n(Sec. A).\\n‚Ä¢The detailed results of the preliminary experiments reported in main paper‚Äôs Fig. 4(Sec. B).\\n‚Ä¢ A more comprehensive comparison with more metrics and baselines (Sec. C).\\n‚Ä¢ Qualitative visualizations (Sec. D).\\n‚Ä¢ Limitation discussion (Sec. E).\\n‚Ä¢ Broader Impacts discussion (Sec. F).\\n‚Ä¢ Dataset Licenses (Sec. G).'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='‚Ä¢ Dataset Licenses (Sec. G).\\nA Training and Testing Protocols\\nFor image reconstruction ( tokenizer )at preliminary experiments , the training augmentation is\\nconfined to random cropping and flipping, following [ 19]. The training regimen spans a short\\nschedule, featuring a batch size of 256over500ktraining iterations, which correlates to roughly 100\\nepochs on the ImageNet dataset. We employ the AdamW optimizer [ 44] with an initial learning rate'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='of1√ó10‚àí4(with cosine decay) and weight decay 1√ó10‚àí4. We only adopt stage-1 training here\\n(i.e., only the ‚Äúwarm-up‚Äù training stage). For the main experiments , we adopt the improvements\\nas shown in the ablation study (Tab. 3 in main paper), including longer training to 200epochs and\\ndecoder fine-tuning, all other hyper-parameters remain the same. We use patch size 16for all vision\\ntransformers at resolution 256√ó256and increase it to 32for resolution 512√ó512to ensure a\\ncomputation efficiency.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='computation efficiency.\\nTiTok-L refers to using a ViT-L for TiTok encoder and decoder, and TiTok-B, TiTok-S refers to using\\nViT-B and ViT-S respectively. Moreover, the tokenizer training takes 64 A100-40G for 74 hours\\n(TiTok-L-32), 32 A100-40G for 41 hours (TiTok-B-64), 32 A100-40G for 50 hours (TiTok-S-128), 32\\nA100-40G for 70 hours (TiTok-B-128 for resolution 512), and 64 A100-40G for 91 hours (TiTok-L-64\\nfor resolution 512), respectively.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='for resolution 512), respectively.\\nFor image generation ( generator )at preliminary experiments , we majorly build the training and\\ntesting protocols on top of [ 9]. Specifically, all images are pre-tokenized using center crop and\\nrandom flipping augmentation, and then processed by MaskGIT [ 9] to generate images via the\\nmasked image modeling procedure. During inference, a cosine masking schedule is utilized with 8'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='steps. The generative models are trained with a batch size of 2048 and500kiterations to improve\\ntraining efficiency. We use AdamW optimizer [ 44] with learning rate 2√ó10‚àí4and weight decay\\n0.03. The learning rate starts from 2√ó10‚àí4and then decay to 1√ó10‚àí5following a cosine decaying\\nschedule. We apply a dropout probability of 0.1on the class condition. The only differences of\\nmain experiments are using an arccos masking schedule as discussed in the ablation study (main'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='paper Tab. 3), all other hyper-parameters remain the same. We follow prior arts [ 19,9] to generate\\n50ksamples for generation FID evaluation. We also adopt classifier-free guidance [ 28] following\\nprior arts [10, 68].\\nAt ImageNet 256√ó256, we use guidance scale 4.5, temperature 9.5 for TiTok-L-32, guidance scale\\n3.0, temperature 11.0 for TiTok-B-64, guidance scale 2.0, temperature 3.0 for TiTok-S-128. At'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 14}, page_content='ImageNet 512√ó512, we use guidance scale 2.0, temperature 7.5 for TiTok-L-64, guidance scale 2.5,\\ntemperature 6.5 for TiTok-B-128.\\nThe generator training takes 32 A100-40G for 12 hours (TiTok-L-32), 16 hours (TiTok-B-64), 29\\nhours (TiTok-S-128), 26 hours (TiTok-B-128 for resolution 512), 18 hours (TiTok-L-64 for resolution\\n512) respectively.\\nB Detailed Results of Preliminary Experiments\\nWe summarize the detailed results for Fig. 4 of main paper in Tab. 4.\\n15'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='Table 4: Detailed results of preliminary experiments in main paper.\\n(a) reconstruction FID.\\n#token 16 32 64 96 128 192 256\\nTiTok-S 25.3 16.2 9.6 6.9 5.6 4.3 3.7\\nTiTok-B 16.8 9.7 5.9 4.7 3.8 3.2 2.9\\nTiTok-L 13.0 6.6 4.0 3.4 3.0 2.6 2.5(b) generation FID.\\n#token 16 32 64 96 128 192 256\\nTiTok-S 16.1 8.7 4.4 3.6 3.5 3.0 3.2\\nTiTok-B 10.9 7.5 4.1 3.4 3.2 3.1 3.5\\nTiTok-L 9.0 6.3 4.3 3.9 3.7 3.7 3.7\\n(c) Linear Probing accuracy.\\n#token 16 32 64 96 128 192 256'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='#token 16 32 64 96 128 192 256\\nTiTok-S 50.46 48.01 48.40 48.12 46.55 47.05 44.88\\nTiTok-B 57.70 54.79 53.92 53.72 53.59 51.75 52.11\\nTiTok-L 62.10 60.03 58.85 56.12 54.35 53.95 54.36\\nTable 5: ImageNet-1K 256√ó256generation results evaluated with ADM [ 16].‚Ä†: Trained on\\nOpenImages [ 35]‚Ä°: Trained on OpenImages, LAION-Aesthetics/-Humans [ 56]. P: generator‚Äôs\\nparameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision,'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='measured with w/ guidance variants if available. ‚Äúguidance\" refers to classifier-free guidance.\\ntokenizer rFID ‚Üì generatorw/o guidance w/ guidanceP‚Üì S‚ÜìT‚ÜëgFID‚ÜìIS‚ÜëgFID‚ÜìIS‚Üë\\ndiffusion-based generative models\\nTaming-VQGAN‚Ä† [55] 1.14 LDM-8 [55] 15.82 78.82 7.76 209.5 258M 200 -\\nV AE‚Ä† [55] 0.27 LDM-4 [55] 10.56 103.5 3.60 247.7 400M 250 0.4\\nV AE [57]‚Ä° 0.62UViT-L/2 [4] 9.03 111.5 3.40 219.9 287M 50 1.1\\nUViT-H/2 [4] 6.60 142.5 2.29 263.9 501M 50 0.6\\nDiT-XL/2 [49] 9.62 121.5 2.27 278.2 675M 250 0.6'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='DiT-XL/2 [49] 9.62 121.5 2.27 278.2 675M 250 0.6\\ntransformer-based generative models\\nTaming-VQGAN [19] 7.94 Taming-Transformer [19] 15.78 78.3 - - 1.4B 256 7.5\\nRQ-V AE [36] 3.20 RQ-Transformer [36]8.71 119.0 - - 1.4B6416.1\\n7.55 134.0 - - 3.8B 9.7\\nMaskGIT-VQGAN [9] 2.28 MaskGIT-ViT [9] 6.18 182.1 - - 177M 8 50.5\\nViT-VQGAN [65] 1.28 VIM-Large [65] 4.17 175.1 - - 1.7B 1024 0.3\\nLFQ [68] ‚àº0.9 MAGVIT-v2 [68] 3.65 200.5 1.78 319.4 307M 64 1.1'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='TiTok-L-32 2.21 MaskGIT-ViT [9] 3.15 173.0 2.77 199.8 177M 8 101.6\\nTiTok-B-64 1.70 MaskGIT-ViT [9] 3.08 192.5 2.48 214.7 177M 8 89.8\\nTiTok-S-128 1.71 MaskGIT-UViT-L [9, 4]4.61 166.7 2.50 278.7287M853.3\\n4.44 168.2 1.97 281.8 64 7.8\\nC Additional Results\\nWe further report the class-conditional generation results comparison with more metrics and baselines\\nin Tab. 5 and Tab. 6 for ImageNet 256√ó256and512√ó512generation benchmarks, respectively.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='Moreover, we report both results without and with classifier-free guidance [ 28] under column ‚Äúw/o\\nguidance\" and ‚Äúw/ guidance\" respectively.\\nAs shown in Tab. 5, both TiTok-L-32 and TiTok-B-64 set a new state-of-the-art performance for\\nresults without classifier-free guidance ( i.e., w/o guidance column), while generating images at a\\nmuch faster pace. Specifically, TiTok-L-32 achieves 3.15gFID, surpassing current state-of-the-art'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='MAGVIT-v2 [ 68]‚Äôs gFID 3.65, while requiring much fewer sampling steps ( 8vs.64) and smaller\\nmodel size ( 177Mvs.307M), leading to a substantial sampling speed-up ( 92.4√ófaster, 101.6vs.\\n1.1samples/sec). Additionally, when compared to MaskGIT [ 9], which uses the exact same generator\\nmodel ( i.e., MaskGIT-ViT) as ours and the only difference is the toeknizer, TiTok-L-32 achieves\\nsignificantly a better performance ( 3.15vs.6.18). The improvement demonstrates the efficiency and'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 15}, page_content='effectiveness of the learned compact 1D latent space for image representation. When it comes to\\nresolution 512√ó512in Tab. 6, MaskGIT [ 9] requires 1024 tokens for image latent representation,\\nwhile TiTok-L-64 requires 16√ófewer. As a result, when using the same generator ( i.e., MaskGIT-\\nViT), TiTok-L-64, w/o classifier-free guidance, not only significantly outperforms MaskGIT in terms\\nof gFID ( 3.64vs.7.32) but also generates samples much faster. The advantages of TiTok become\\n16'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='Table 6: ImageNet-1K 512√ó512generation results evaluated with ADM [ 16].‚Ä†: Trained on\\nOpenImages [ 35]‚Ä°: Trained on OpenImages, LAION-Aesthetics/-Humans [ 56]. P: generator‚Äôs\\nparameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision,\\nmeasured with w/ guidance variants if available. ‚Äúguidance\" refers to classifier-free guidance.\\ntokenizer rFID ‚Üì generatorw/o guidance w/ guidanceP‚ÜìS‚ÜìT‚ÜëgFID‚ÜìIS‚ÜëgFID‚ÜìIS‚Üë\\ndiffusion-based generative models'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='diffusion-based generative models\\nV AE [57]‚Ä° 0.19UViT-L/4 [4] 18.03 76.9 4.67 213.3 287M 50 1.0\\nUViT-H/4 [4] 15.71 101.3 4.05 263.8 501M 50 0.6\\nDiT-XL/2 [49] 12.03 105.3 3.04 240.8 675M 250 0.1\\ntransformer-based generative models\\nMaskGIT-VQGAN [9] 1.97 MaskGIT-ViT [9] 7.32 156.0 - - 177M 12 3.9\\nLFQ [68] 1.22 MAGVIT-v2 [68]4.61 192.4 - - 307M 12 3.5\\n3.07 213.1 1.91 324.3 307M 64 1.0\\nTiTok-L-64 1.78 MaskGIT-ViT [9] 3.64 179.8 2.74 221.1 177M 8 41.0'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='TiTok-B-128 1.37 MaskGIT-ViT [9]3.91 182.0 2.49 260.4177M833.3\\n4.17 181.0 2.13 261.2 64 7.4\\neven more significant when compared to the diffusion models such as DiT-XL/2 [ 49] with guidance:\\nTiTok-L-64 not only shows a superior performance ( 2.74vs.3.04), but also enjoys a dramatically\\nhigher generation throughput ( 410√ó).\\nAn interesting observation is that under w/o guidance case, TiTok-L-32 (for 256 resolution) and'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='TiTok-L-64 (for 512 resolution) can outperform most other methods, including TiTok-S-128 and\\nTiTok-B-128, yet they benefit relatively less from the classifier-free guidance in the w/ guidance\\ncolumn. We note it indicates that the great potential of TiTok at compact latent size is still not fully\\nunleashed yet, and better adaptation of inference time improvements for 1D compact tokens, such as\\nclassifier-free guidance, which was designed for methods with much more tokens and steps, could be'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='a promising future direction.\\nD Visualizations\\nWe provide visualization of the generated images using TiTok in Fig. 5 and Fig. 6. Moreover, we\\nvisualize the reconstruction results under different numbers of tokens and different model sizes\\nin Fig. 7, where we observe that the model tends to keep the high-level layout or salient objects when\\nthe latent representation size is limited. Besides, a larger model size reconstructs an image with more'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='details under a compact latent space size, demonstrating an effective way towards a more compact\\nlatent space.\\nE Limitations\\nThis paper proposes a novel 1D tokenization method designed to eliminate the fixed corresponding\\nconstraints of existing 2D tokenization methods. The 1D tokenization model is validated using\\nthe Vector Quantization (VQ) tokenizer formulation alongside a Masked Transformer generator'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='framework. Despite the promising results, the proposed 1D tokenization formulation theoretically\\nhas the potential to generalize to other tokenizer formulations ( e.g., 1D-V AE), other generation\\nframeworks ( e.g., Diffusion Models), and beyond the image modality ( e.g., video). However,\\nexploring these extensions is beyond the scope of this paper due to limited computational resources,\\nand we leave these as promising directions for future research.\\nF Broader Impacts'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='F Broader Impacts\\nGenerative models have numerous applications with diverse potential social impacts. While these\\nmodels significantly enhance human creativity, they can also be misused for misinformation, harass-\\nment, and perpetuating social and cultural biases. Similar to other deep learning methods, generative\\nmodels can be heavily influenced by dataset biases, leading to the reinforcement of negative social'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 16}, page_content='stereotypes and viewpoints. Developing unbiased models that ensure both robustness and fairness\\n17'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 17}, page_content=\"macawlionjack-o'-lanternorangedaisybubblevalleyTiTok-L-32TiTok-B-64TiTok-S-128Figure 5: Visualization of generated images from TiTok variants with MaskGIT [ 9].Correspond-\\ning ImageNet class names are shown below the images.\\nFigure 6: Visualization of generated images from TiTok-L-32 with MaskGIT [ 9] across random\\nImageNet classes.\\nis a critical area of research. However, addressing these issues is beyond the scope of this paper.\"),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 17}, page_content='Considering the potential risks, this paper is limited to class-conditional generation using a fixed,\\npublic, and controlled set of classes.\\n18'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 18}, page_content='Increasing model size (S, B, L)Increasing number of latent token (K=16, 32, 64, 128, 256)\\nFigure 7: Visual comparison of reconstruction results. Scaling model size enables a better image\\nquality while using a more compact latent space size. It is also observed that TiTok tends to keep the\\nsalient regions when latent space is limited.\\nG Dataset Licenses\\nThe datasets we used for training and/or testing TiTok are described as follows.'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 18}, page_content='ImageNet-1K: We train and evaluate TiTok on ImageNet-1K generation benchmark. This dataset\\nspans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and\\n100,000 test images. We use the training set for our tokenizer and generator training. The validation\\n19'),\n",
       " Document(metadata={'source': '/root/users/jusjus/Self/2406.07550v1.pdf', 'page': 19}, page_content='set is used to compute reconstruction FID for evaluating tokenizers. The generation results are\\nevaluated with generation FID using pre-computed statistics and scripts from ADM [16]3.\\nLicense: https://image-net.org/accessagreement\\nURL: https://www.image-net.org/\\n3https://github.com/openai/guided-diffusion/tree/main/evaluations\\n20')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1398203/3322674952.py:39: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceEmbeddings(\n",
      "/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://arxiv.org/pdf/2406.07550\"\n",
    "]\n",
    "\"\"\"urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "\n",
    "# Read Web doc\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\"\"\"\n",
    "\n",
    "# Read local pdf\n",
    "DOC_PATH = \"/root/users/jusjus/Self/apple_10K.pdf\"\n",
    "loader = PyPDFLoader(DOC_PATH)\n",
    "pages = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Add to Milvus\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus\",\n",
    "    embedding=hf,\n",
    "    connection_args={\"uri\": \"./milvus_rag.db\"},\n",
    "\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"document\": \"Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"document\": \"Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n    \\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\\n     \\n    Here is the retrieved document: \\n    Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n    \\n    Here is the user question: \\n    agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [188ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-10-15T02:59:49.058613691Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 185590767,\n",
      "          \"load_duration\": 19978985,\n",
      "          \"prompt_eval_count\": 360,\n",
      "          \"prompt_eval_duration\": 103722000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 60936000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-10-15T02:59:49.058613691Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 185590767,\n",
      "              \"load_duration\": 19978985,\n",
      "              \"prompt_eval_count\": 360,\n",
      "              \"prompt_eval_duration\": 103722000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 60936000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-11caa957-37e1-42cb-a774-eb003e6b99f8-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [190ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing the relevance \n",
    "    of a retrieved document to a financial auditing question. If the document contains financial data, key figures, or information \n",
    "    that directly supports the audit objective, grade it as relevant. Use domain knowledge of financial auditing to assess the match.\n",
    "    \n",
    "    Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here is the retrieved document: \n",
    "    {document}\n",
    "    \n",
    "    Here is the auditing question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. \\n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n    Use three sentences maximum and keep the answer concise:\\n    Question: agent memory \\n    Context: [Document(metadata={'pk': 453236105473687587, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent‚Äôs behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent‚Äôs future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'pk': 453236105473687609, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\\\n\\\\n\\\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\\\n\\\\n\\\\nCitation#\\\\nCited as:'), Document(metadata={'pk': 453236105473687552, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\\\"LLM Powered Autonomous Agents | Lil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nPosts\\\\n\\\\n\\\\n\\\\n\\\\nArchive\\\\n\\\\n\\\\n\\\\n\\\\nSearch\\\\n\\\\n\\\\n\\\\n\\\\nTags\\\\n\\\\n\\\\n\\\\n\\\\nFAQ\\\\n\\\\n\\\\n\\\\n\\\\nemojisearch.app\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n      LLM Powered Autonomous Agents\\\\n    \\\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\\\n\\\\n\\\\n \\\\n\\\\n\\\\nTable of Contents\\\\n\\\\n\\\\n\\\\nAgent System Overview\\\\n\\\\nComponent One: Planning\\\\n\\\\nTask Decomposition\\\\n\\\\nSelf-Reflection\\\\n\\\\n\\\\nComponent Two: Memory\\\\n\\\\nTypes of Memory\\\\n\\\\nMaximum Inner Product Search (MIPS)\\\\n\\\\n\\\\nComponent Three: Tool Use\\\\n\\\\nCase Studies\\\\n\\\\nScientific Discovery Agent\\\\n\\\\nGenerative Agents Simulation\\\\n\\\\nProof-of-Concept Examples\\\\n\\\\n\\\\nChallenges\\\\n\\\\nCitation\\\\n\\\\nReferences\\\"), Document(metadata={'pk': 453236105473687586, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\\\nGenerative Agents Simulation#\\\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\\\n\\\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents‚Äô experience in natural language.')] \\n    Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [700ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-10-15T02:59:52.321886383Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 697818209,\n",
      "          \"load_duration\": 18862331,\n",
      "          \"prompt_eval_count\": 1153,\n",
      "          \"prompt_eval_duration\": 291769000,\n",
      "          \"eval_count\": 46,\n",
      "          \"eval_duration\": 386303000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-10-15T02:59:52.321886383Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 697818209,\n",
      "              \"load_duration\": 18862331,\n",
      "              \"prompt_eval_count\": 1153,\n",
      "              \"prompt_eval_duration\": 291769000,\n",
      "              \"eval_count\": 46,\n",
      "              \"eval_duration\": 386303000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ec889db1-446b-41b0-ac3f-83dfd5c02196-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [702ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\"\n",
      "}\n",
      "The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant specializing in financial auditing. \n",
    "    Use the following financial data and contextual information to provide a concise and accurate response to the question. If the information is insufficient, \n",
    "    state that you do not have enough data to answer the question.\n",
    "    \n",
    "    Use three sentences maximum and keep the answer precise:\n",
    "    Auditing Question: {question} \n",
    "    Financial Data Context: {context} \n",
    "    Auditor's Response: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether \\n    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \\n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \\n    single key 'score' and no preamble or explanation.\\n    \\n    Here are the facts:\\n    [Document(metadata={'pk': 453236105473687587, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Each element is an observation, an event directly provided by the agent.\\\\n- Inter-agent communication can trigger new natural language statements.\\\\n\\\\n\\\\nRetrieval model: surfaces the context to inform the agent‚Äôs behavior, according to relevance, recency and importance.\\\\n\\\\nRecency: recent events have higher scores\\\\nImportance: distinguish mundane from core memories. Ask LM directly.\\\\nRelevance: based on how related it is to the current situation / query.\\\\n\\\\n\\\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent‚Äôs future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\\\n\\\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\\\n\\\\n\\\\nPlanning & Reacting: translate the reflections and the environment information into actions'), Document(metadata={'pk': 453236105473687609, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\\\n\\\\n\\\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\\\n\\\\n\\\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\\\n\\\\n\\\\nCitation#\\\\nCited as:'), Document(metadata={'pk': 453236105473687552, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\\\"LLM Powered Autonomous Agents | Lil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLil'Log\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nPosts\\\\n\\\\n\\\\n\\\\n\\\\nArchive\\\\n\\\\n\\\\n\\\\n\\\\nSearch\\\\n\\\\n\\\\n\\\\n\\\\nTags\\\\n\\\\n\\\\n\\\\n\\\\nFAQ\\\\n\\\\n\\\\n\\\\n\\\\nemojisearch.app\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n      LLM Powered Autonomous Agents\\\\n    \\\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\\\n\\\\n\\\\n \\\\n\\\\n\\\\nTable of Contents\\\\n\\\\n\\\\n\\\\nAgent System Overview\\\\n\\\\nComponent One: Planning\\\\n\\\\nTask Decomposition\\\\n\\\\nSelf-Reflection\\\\n\\\\n\\\\nComponent Two: Memory\\\\n\\\\nTypes of Memory\\\\n\\\\nMaximum Inner Product Search (MIPS)\\\\n\\\\n\\\\nComponent Three: Tool Use\\\\n\\\\nCase Studies\\\\n\\\\nScientific Discovery Agent\\\\n\\\\nGenerative Agents Simulation\\\\n\\\\nProof-of-Concept Examples\\\\n\\\\n\\\\nChallenges\\\\n\\\\nCitation\\\\n\\\\nReferences\\\"), Document(metadata={'pk': 453236105473687586, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\\\nGenerative Agents Simulation#\\\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\\\n\\\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents‚Äô experience in natural language.')] \\n\\n    Here is the answer: \\n    The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [389ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-10-15T02:59:54.989834802Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 385614468,\n",
      "          \"load_duration\": 20554987,\n",
      "          \"prompt_eval_count\": 1221,\n",
      "          \"prompt_eval_duration\": 310078000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 53988000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-10-15T02:59:54.989834802Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 385614468,\n",
      "              \"load_duration\": 20554987,\n",
      "              \"prompt_eval_count\": 1221,\n",
      "              \"prompt_eval_duration\": 310078000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 53988000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-fc9c08ec-3098-4f5f-abac-a2234a58de44-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [391ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether \n",
    "    an auditor's response is grounded in the provided financial data or audit evidence. Determine if the response directly references or is supported by the data.\n",
    "    \n",
    "    Give a binary score 'yes' or 'no' to indicate whether the response is grounded in the provided evidence. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here is the audit evidence:\n",
    "    {documents} \n",
    "    \n",
    "    Here is the auditor's response: \n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"generation\": \"The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"agent memory\",\n",
      "  \"generation\": \"The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a grader assessing whether an \\n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \\n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\\n     \\n    Here is the answer:\\n    The agent's memory is informed by the retrieval model, which surfaces context based on relevance, recency, and importance. The reflection mechanism synthesizes memories into higher-level inferences over time to guide the agent's future behavior. \\n\\n    Here is the question: agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [117ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-10-15T02:59:56.82341023Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 113997672,\n",
      "          \"load_duration\": 18965311,\n",
      "          \"prompt_eval_count\": 137,\n",
      "          \"prompt_eval_duration\": 38049000,\n",
      "          \"eval_count\": 7,\n",
      "          \"eval_duration\": 56090000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"score\\\": \\\"yes\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-10-15T02:59:56.82341023Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 113997672,\n",
      "              \"load_duration\": 18965311,\n",
      "              \"prompt_eval_count\": 137,\n",
      "              \"prompt_eval_duration\": 38049000,\n",
      "              \"eval_count\": 7,\n",
      "              \"eval_duration\": 56090000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-867f5f7c-38a6-4118-a898-da59de03b145-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [119ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"score\": \"yes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader \n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an \n",
    "    auditor's response effectively addresses the auditing question. Use your knowledge of financial auditing to evaluate if the response is useful \n",
    "    and actionable for resolving the question.\n",
    "    \n",
    "    Give a binary score 'yes' or 'no' to indicate whether the response is useful. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here is the auditing question: \n",
    "    {question}\n",
    "    \n",
    "    Here is the auditor's response:\n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1398203/1816432062.py:8: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
      "/tmp/ipykernel_1398203/1816432062.py:25: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"llm agent memory\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    llm agent memory\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [3.02s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-10-15T02:59:34.542586188Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3013897263,\n",
      "          \"load_duration\": 2867113143,\n",
      "          \"prompt_eval_count\": 131,\n",
      "          \"prompt_eval_duration\": 61772000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 84018000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"vectorstore\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-10-15T02:59:34.542586188Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 3013897263,\n",
      "              \"load_duration\": 2867113143,\n",
      "              \"prompt_eval_count\": 131,\n",
      "              \"prompt_eval_duration\": 61772000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 84018000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3a7402f2-bb3c-4dae-ba7e-bfb546d771a0-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.02s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"vectorstore\"\n",
      "}\n",
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert in financial auditing. \n",
    "    Route the user's question to the appropriate data source based on its context. Use 'financial_reports' for questions about balance sheets, income statements, or audit procedures. \n",
    "    Use 'external_sources' for questions requiring external benchmarks or general audit standards.\n",
    "    \n",
    "    Provide a binary choice 'financial_reports' or 'external_sources' based on the question. Return the result as a JSON with a single key 'datasource' and no preamble or explanation.\n",
    "    \n",
    "    Here is the auditing question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f80ec5dfa60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    web_search : str\n",
    "    documents : List[str]\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "    \n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})  \n",
    "    print(source)\n",
    "    print(source['datasource'])\n",
    "    if source['datasource'] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f80ec5dfa60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<question,generation,web_search,documents>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "---ROUTE QUESTION---\n",
      "Did Emmanuel Macron visit Germany recently?\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an expert at routing a \\n    user question to a vectorstore or web search. Use the vectorstore for questions on LLM  agents, \\n    prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords \\n    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \\n    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \\n    no premable or explaination. \\n    \\n    Question to route: \\n    Did Emmanuel Macron visit Germany recently?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > llm:ChatOllama] [110ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\\"datasource\\\": \\\"web_search\\\"}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-10-15T03:30:08.186708314Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 107536494,\n",
      "          \"load_duration\": 20725455,\n",
      "          \"prompt_eval_count\": 134,\n",
      "          \"prompt_eval_duration\": 10478000,\n",
      "          \"eval_count\": 9,\n",
      "          \"eval_duration\": 75469000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\\"datasource\\\": \\\"web_search\\\"}\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-10-15T03:30:08.186708314Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 107536494,\n",
      "              \"load_duration\": 20725455,\n",
      "              \"prompt_eval_count\": 134,\n",
      "              \"prompt_eval_duration\": 10478000,\n",
      "              \"eval_count\": 9,\n",
      "              \"eval_duration\": 75469000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-1e56af5b-a242-4554-a887-f042ac8a6cfe-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence > parser:JsonOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question > chain:RunnableSequence] [113ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"datasource\": \"web_search\"\n",
      "}\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:route_question] [113ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"websearch\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [113ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Did Emmanuel Macron visit Germany recently?\"\n",
      "}\n",
      "---WEB SEARCH---\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:LangGraph > chain:websearch] [3ms] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('documents')Traceback (most recent call last):\\n\\n\\n  File \\\"/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/utils/runnable.py\\\", line 410, in invoke\\n    input = context.run(step.invoke, input, config, **kwargs)\\n\\n\\n  File \\\"/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/utils/runnable.py\\\", line 184, in invoke\\n    ret = context.run(self.func, input, **kwargs)\\n\\n\\n  File \\\"/tmp/ipykernel_1398203/2830740018.py\\\", line 108, in web_search\\n    documents = state[\\\"documents\\\"]\\n\\n\\nKeyError: 'documents'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[chain:LangGraph] [119ms] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('documents')Traceback (most recent call last):\\n\\n\\n  File \\\"/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 1273, in stream\\n    for _ in runner.tick(\\n\\n\\n  File \\\"/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/pregel/runner.py\\\", line 56, in tick\\n    run_with_retry(t, retry_policy)\\n\\n\\n  File \\\"/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/pregel/retry.py\\\", line 29, in run_with_retry\\n    task.proc.invoke(task.input, config)\\n\\n\\n  File \\\"/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/utils/runnable.py\\\", line 410, in invoke\\n    input = context.run(step.invoke, input, config, **kwargs)\\n\\n\\n  File \\\"/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/utils/runnable.py\\\", line 184, in invoke\\n    ret = context.run(self.func, input, **kwargs)\\n\\n\\n  File \\\"/tmp/ipykernel_1398203/2830740018.py\\\", line 108, in web_search\\n    documents = state[\\\"documents\\\"]\\n\\n\\nKeyError: 'documents'\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid Emmanuel Macron visit Germany recently?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m         pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1273\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1268\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1269\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1270\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1271\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1272\u001b[0m     ):\n\u001b[0;32m-> 1273\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1274\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1275\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1276\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1277\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1278\u001b[0m         ):\n\u001b[1;32m   1279\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/mnt/znzz/anaconda3/pkgs/jusjus/lib/python3.9/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[21], line 108\u001b[0m, in \u001b[0;36mweb_search\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---WEB SEARCH---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m question \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Web search\u001b[39;00m\n\u001b[1;32m    111\u001b[0m docs \u001b[38;5;241m=\u001b[39m web_search_tool\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n",
      "\u001b[0;31mKeyError\u001b[0m: 'documents'"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Did Emmanuel Macron visit Germany recently?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
